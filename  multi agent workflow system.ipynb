{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqKXjoxkTzipxjfK5LufK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiloufarYousefi/-Final-project/blob/main/%20multi%20agent%20workflow%20system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pti2J-cSPz6r",
        "outputId": "652f69fb-f9ba-41b1-fb67-922584837edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.4-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting diskcache (from autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from autogen)\n",
            "  Downloading FLAML-2.3.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.54.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen) (24.2)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen) (2.9.2)\n",
            "Collecting python-dotenv (from autogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen) (2.5.0)\n",
            "Collecting tiktoken (from autogen)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.26.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (2.23.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen) (2024.9.11)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen) (3.4.0)\n",
            "Downloading autogen-0.4-py3-none-any.whl (366 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.2/366.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.2-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, flaml, diskcache, tiktoken, docker, autogen\n",
            "Successfully installed autogen-0.4 diskcache-5.6.3 docker-7.1.0 flaml-2.3.2 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Post #1:\n",
            "Flagging Results:\n",
            "None: safe\n",
            "Result: Content Safe\n",
            "\n",
            "Processing Post #2:\n",
            "Flagging Results:\n",
            "None: safe\n",
            "Result: Content Safe\n",
            "\n",
            "Processing Post #3:\n",
            "Flagging Results:\n",
            "None: safe\n",
            "Result: Content Safe\n",
            "\n",
            "Processing Post #4:\n",
            "Flagging Results:\n",
            "None: flagged\n",
            "Result: Content Flagged\n",
            "\n",
            "Processing Post #5:\n",
            "Flagging Results:\n",
            "None: safe\n",
            "Result: Content Safe\n"
          ]
        }
      ],
      "source": [
        "# Install AutoGen\n",
        "!pip install autogen\n",
        "\n",
        "# Import necessary libraries\n",
        "import autogen\n",
        "from autogen import Agent\n",
        "\n",
        "# Define the agents with run methods\n",
        "class ViolenceAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"ViolenceAgent\")\n",
        "\n",
        "    def run(self, content):\n",
        "        # Basic text-based violence detection (expand as needed)\n",
        "        if \"kill\" in content or \"fight\" in content:\n",
        "            return \"flagged\"\n",
        "        return \"safe\"\n",
        "\n",
        "\n",
        "class HateSpeechAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"HateSpeechAgent\")\n",
        "\n",
        "    def run(self, content):\n",
        "        # Simple hate speech detection\n",
        "        hate_keywords = [\"hate\", \"violence\", \"racist\", \"disgusting\"]\n",
        "        if any(keyword in content for keyword in hate_keywords):\n",
        "            return \"flagged\"\n",
        "        return \"safe\"\n",
        "\n",
        "\n",
        "class SpamAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"SpamAgent\")\n",
        "\n",
        "    def run(self, content):\n",
        "        # Basic spam detection (e.g., promotional content, excessive links)\n",
        "        if \"free\" in content or \"click here\" in content:\n",
        "            return \"flagged\"\n",
        "        return \"safe\"\n",
        "\n",
        "\n",
        "# Workaround to simulate a sequential execution of agents\n",
        "def multi_agent_flagging_system(content):\n",
        "    agents = [ViolenceAgent(), HateSpeechAgent(), SpamAgent()]\n",
        "\n",
        "    flag_results = {}\n",
        "    for agent in agents:\n",
        "        result = agent.run(content)\n",
        "        flag_results[agent.name] = result\n",
        "\n",
        "    print(\"Flagging Results:\")\n",
        "    for agent_name, result in flag_results.items():\n",
        "        print(f\"{agent_name}: {result}\")\n",
        "\n",
        "    # If any agent flags the content, the post is flagged\n",
        "    if \"flagged\" in flag_results.values():\n",
        "        return \"Content Flagged\"\n",
        "    else:\n",
        "        return \"Content Safe\"\n",
        "\n",
        "\n",
        "# Simulate some social media posts for flagging\n",
        "posts = [\n",
        "    \"Check out this amazing video of a tiger hunting a deer. It's incredible to watch nature in action!\",\n",
        "    \"This is the best way to solve all problems, just eliminate the people you don't like! #violence #hatred\",\n",
        "    \"Get rich quick by buying this amazing product. Limited time offer, don't miss out!\",\n",
        "    \"Here's a free service for you! Just click this link and claim your reward.\",\n",
        "    \"This is just a motivational quote about life and success.\"\n",
        "]\n",
        "\n",
        "# Run the flagging system on each post\n",
        "for i, post in enumerate(posts):\n",
        "    print(f\"\\nProcessing Post #{i + 1}:\")\n",
        "    result = multi_agent_flagging_system(post)\n",
        "    print(f\"Result: {result}\")\n"
      ]
    }
  ]
}